your job is to write a program to solve all the coil levels up to the final, which is 2000 by 2000 large. this will require a series of very clever algorithms, repeated experimentation, and many iterations.

evaluation workflow:
- before running evaluations for the first time, build the checker with `make -C coil_check`.
- use `./evaluate.py <solver_program>` for normal development runs. this evaluates odd/public levels only (from `levels_public`).
- use `./evaluate_full.py <solver_program>` only for user-gated full benchmark runs. this prompts for a password and evaluates both odd and even levels.
- do not run `./evaluate_full.py` unless the user explicitly asks for a full benchmark run.
- do not try to access or inspect even-level content directly. do not decrypt or expose hidden levels unless explicitly running the full evaluation command for a benchmark run.
- do not modify benchmark assets or harness during solver iteration unless explicitly asked. this includes `levels_public`, `levels_secret_even.tar.enc`, `evaluate.py`, `evaluate_full.py`, and `coil_check/check.c`.

reporting workflow:
- after any significant improvement, run evaluation from the beginning (`--start 1`) and report that result only (no cherry-picked level ranges).
- evaluation scripts automatically append simple tabular results to `test.md` with columns: date, solver/model, timeout, highest passed, mode, command.
- keep `test.md` append-only from evaluator output (no manual edits to results rows).
- if you discuss score/progress, cite runs that are in `test.md`.
- include reproducibility parameters in any report summary: exact command used, timeout, and start/end range.
- always include the first failing level and one concrete failure reason from checker output (or state complete pass for tested range).

this codebase is intended as a benchmark suite. therefore, do not cheat, do not look for external solutions, do not cache level-specific answers, and do not hardcode solutions for specific levels.
